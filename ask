#!/usr/bin/env python3
# -*- coding:utf8 -*-

import sys
import string

import preprocess as st

class QuestionGenerator:
    """
    Class to generate questions given a string representing a text document.
    """

    def generate_wh_questions(self, doc):
        """
        Generates wh- questions from a document.

        Parameters:
            doc (spaCy Doc object): Input text document parsed by spaCy & neural_coref

        Returns:
            questions (list(tuple)): List of tuples with info about wh-questions
                tuple[0] (str): Question string.
                tuple[1] (str): Class of the question, in
                    ['WHO', 'WHAT', 'WHEN', 'WHERE', 'WHY', 'HOW', 'WHICH, 'BINARY']
                tuple[2] (str): Answer to the question. (or None)
        """

        output = []

        output.extend(self.generate_entity_wh_questions(doc))
        output.extend(self.generate_wh_questions_by_sentence(doc))

        # set to remove duplicates, list to remain subscriptable
        return list(set(output))

    def generate_entity_wh_questions(self, doc):
        """Returns a list of questions for every uniquely identified entity in the doc."""

        output = []

        for cluster in doc._.coref_clusters:
            print(cluster)
            output.append(self.generate_entity_wh_question(cluster))

        return output

    def generate_entity_wh_question(self, coref_cluster):
        """Returns a wh- question created for one entity."""

        entity = coref_cluster.main

        # TODO: get right plurality
        # TODO: truecasing
        #   (https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21)
        #   for now, just using .title() to capitalize every word
        entity_text = entity.text.title().strip()

        # entity[0] gives token, so can call ent_type_
        wh_word = self.wh_word_from(entity[0])

        if wh_word:
            return (wh_word + ' is ' + entity_text + "?", wh_word.upper(), None)

        return None

    def generate_wh_questions_by_sentence(self, doc):
        """
        Returns questions generated by traversing document sentence-by-sentence
        and using spaCy's dependency parser to determine subjects & objects.
        """

        output = []

        for sentence in doc.sents:
            is_question = False
            question = []
            main_verb = None
            main_verb_found = False

            for token in sentence:

                # looking for the nominal subject of the sentence...
                if token.dep_ == 'nsubj' and token.head.pos_ == 'VERB':

                    # check if the subject is the root of a noun chunk
                    # (this prevents us from making duplicate questions for multi-token objects)
                    chunk = next((chunk for chunk in doc.noun_chunks if chunk.root == token), None)
                    if chunk:
                        # try to create a question where the answer is the subject of the sentence
                        subj_q = self.generate_wh_subj_question(chunk)
                        if subj_q:
                            output.append(subj_q)

                    # try to get a wh- pronoun for the subject
                    # FIXME: only gets possible pronouns for named entities. Should be expanded.
                    possible_pronoun = self.wh_word_from(token)

                    # if a pronoun exists, replace the word, mark this as a valid question and begin constructing the question
                    if possible_pronoun:
                        is_question = True
                        main_verb = token.head
                        # TODO: later, will want to append whole token for synonyms. for now, just string.
                        question.append(possible_pronoun)

                # if a nominal subject has been found, start constructing the rest of the question
                if is_question:
                    # waiting till we see the main verb because we don't want to append any extra words from the nsubj's noun chunk
                    if main_verb and token == main_verb:
                        main_verb_found = True

                    # once we've found the main verb, begin appending tokens
                    if main_verb_found:
                        question.append(token.text)

            # construct the object question string and add to output
            if len(question) > 0:
                # indexing till -1 to avoid final punctuation
                out = self.compile_question_string(question[:-1])

                output.append(out)

        return output

    def generate_wh_subj_question(self, chunk):
        """Returns a question where the answer is the subject of the sentence."""

        # Exclude questions and "who"
        if chunk.root.tag_ in ["PRP", "WP"]:
            return None

        wh_word = self.wh_word_from(chunk.root)
        # TODO: check if word is part of coref cluster, use the root entity type for
        #       wh- word
        if not wh_word:
            wh_word = "What" # default

        # change 'to be' inflection for plural words
        # spacy.Token.tag denotes plural words with a final 'S'
        if chunk.root.tag_.endswith('S'):
            linking_verb = 'are'
        else:
            linking_verb = 'is'

        # only capitalize proper names
        # using non-null entity types as a proxy
        if chunk.root.ent_type_ == '':
            text = chunk.text.lower()
        else:
            text = chunk.text

        return wh_word + ' ' + linking_verb + " " + text.strip() + "?"

    def wh_word_from(self, subj):
        """Returns a wh-word based on the entity type of subj."""

        # dictionary to map from entity types to wh- pronouns
        switcher = {
            'PERSON': 'Who',
            'GPE': 'What',
            'LOC': 'Where',
            'ORG': 'What',
            'FC': 'What',
            'EVENT': 'What',
            'WORK_OF_ART': 'What',
        }

        candidate = switcher.get(subj.ent_type_, None)

        if candidate:
            return candidate
        
        if subj._.in_coref:
            print(subj, end= " ")
            print(subj._.coref_clusters[0].main[0].ent_type_)
            return switcher.get(subj._.coref_clusters[0].main[0].ent_type_, None)

        return None

    def compile_question_string(self, lst):
        """Modifies punctuation to create a grammatical question string."""

        output = lst[0]

        num_quotes = 0

        no_space_after = set(['$', '(', '[', '{', '-'])

        no_space_before = set(string.punctuation) - set(['$', '(', '[', '{', '"'])

        for i in range(1, len(lst)):
            token = lst[i]
            last = lst[i-1]

            if token == '"':
                num_quotes += 1

                if num_quotes % 2 != 0:
                    output += ' '

            elif last not in no_space_after and token[0] not in no_space_before:
                if last != '"' or num_quotes% 2 == 0:
                    output += ' '

            output += token.strip()


        if output[-1] not in ['"', "'"] and output[-1] in string.punctuation:
            output = output[:-1]

        output += '?'

        return output

def main():
    """
    Main run function; parses a text document and
    prints generated questions to stdout.
    """

    # Ensure 2 arguments
    if len(sys.argv) != 3:
        print("Usage: ./ask ARTICLE_TXT NUM_QUESTIONS")
        sys.exit(1)

    # Read string from 1 text file.
    input_txt = sys.argv[1]

    # TODO: validate that input_txt conforms to utf-8 encoding.

    # validate that n_questions is an integer
    if not sys.argv[2].isdigit():
        print("NUM_QUESTIONS must be an integer")
        sys.exit(1)

    n_questions = int(sys.argv[2])

    with open(input_txt, 'r') as file:
        text = file.read()

    # Instantiate our preprocessor and get the processed doc
    preprocesser = st.Preprocessor(text)
    processed_doc = preprocesser.doc

    # Instantiate our question generator and make some questions
    question_generator = QuestionGenerator()

    # Final output
    wh_questions = question_generator.generate_wh_questions(processed_doc)
    for i in range(n_questions):
        j = i % len(wh_questions)
        print(wh_questions[j])
    # /Final output

    # DEBUG; remove for prod
    print()
    for output_question in wh_questions:
        print(output_question)
    # /DEBUG

if __name__ == "__main__":
    main()
